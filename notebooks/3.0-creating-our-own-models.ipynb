{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making an RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(61424) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# Importing utility functions from Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import SimpleRNN, Dense, LSTM, Flatten\n",
    "from keras.layers import Input, Dense, LSTM, Attention, Concatenate\n",
    "\n",
    "from utils import tokenize_song, tokenize_song_by_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GRAM = 5\n",
    "EMBEDDING_SIZE = 100\n",
    "BATCH_SIZE = 1000\n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "PROCESSED_DATA_FILE = \"../data/processed/processed_data.csv\"\n",
    "STANZAS_FILE = '../data/processed/stanzas.txt'\n",
    "EMBEDDING_FILE = \"../reference-materials/lyrics_embeddings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stanzas_as_words = []\n",
    "with open(STANZAS_FILE, 'r', encoding='utf-8') as txtfile:\n",
    "    for line in txtfile:\n",
    "        # Split each line into a list using '\\t' as the separator\n",
    "        line_data = line.strip().split('\\t')\n",
    "        new_stanzas_as_words.append(line_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of song lyrics with genre\n",
    "Tokenizes the each song into a a list of sentences. Appends the genre of the song in front of each \n",
    "sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(new_stanzas_as_words)\n",
    "# Convert stanzas into numerical indexes (list of lists of string -> list of lists of int)\n",
    "stanzas = tokenizer.texts_to_sequences(new_stanzas_as_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  87923\n"
     ]
    }
   ],
   "source": [
    "# print size of vocab\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the embeddings and create dictionary mapping index to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> dict:\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    index_to_embedding = {}  # Mapping from index to its embedding vector\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            split_line = line.split()\n",
    "            # Skip the first line of file\n",
    "            if len(split_line) == 2:\n",
    "                continue\n",
    "            word = split_line[0]\n",
    "            vector = [float(x) for x in split_line[1:]]\n",
    "        \n",
    "            if word in tokenizer.word_index:\n",
    "                index_to_embedding[tokenizer.word_index[word]] = vector # Mapping from index to its embedding vector\n",
    "    return index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_embedding = read_embeddings(EMBEDDING_FILE, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ngram training samples\n",
    "Add each genre to the beginning of the ngram. The ngram is of size 5, index 0 is the genre. Indexes\n",
    "1-3 are the features, and index 5 is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int):\n",
    "    \"\"\"\n",
    "    Generates n-gram training samples from a list of encoded words. \n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    ngram = ngram - 2\n",
    "    for lyric in encoded:\n",
    "      for i in range(1, len(lyric) - ngram):\n",
    "          X.append([lyric[0]] + lyric[i:i + ngram])\n",
    "          y.append(lyric[i + ngram])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_ngram_training_samples(stanzas, N_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  14990276\n",
      "Number of labels:  14990276\n",
      "First training sample:  [530, 13, 13, 13]\n",
      "First label:  541\n",
      "Second training sample:  [530, 13, 13, 541]\n",
      "Second label:  11\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples: \", len(X))\n",
    "print(\"Number of labels: \", len(y))\n",
    "print(\"First training sample: \", X[0])\n",
    "print(\"First label: \", y[0])\n",
    "print(\"Second training sample: \", X[1])\n",
    "print(\"Second label: \", y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Generator for Models\n",
    "Creates a data genator which yields in batches feature embeddings and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSamplesToEmbeddings(samples: list, index_to_embedding: dict):\n",
    "    \"\"\"\n",
    "    Converts a list of samples to a list of embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for sample in samples:\n",
    "        embedding = []\n",
    "        for word in sample:\n",
    "            embedding.append(index_to_embedding[word])\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate batches of data\n",
    "def data_generator(data, labels, index_to_embedding, batch_size, sequence_length, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        num_batches = len(data) // batch_size\n",
    "        while True:\n",
    "            for i in range(num_batches):\n",
    "                batch_data = data[i: i + batch_size]\n",
    "                batch_labels = labels[i: i + batch_size]\n",
    "                batch_data = convertSamplesToEmbeddings(batch_data, index_to_embedding)\n",
    "                batch_labels = [to_categorical(label, num_classes=len(index_to_embedding)) for label in batch_labels]\n",
    "                yield np.array(batch_data), np.array(batch_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.7 * len(X))\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feed_forward_model(X, y, index_to_embedding: dict, batch_size=1000, sequence_length=N_GRAM, epochs=1):\n",
    "  generator = data_generator(X, y, index_to_embedding, batch_size, sequence_length, epochs)\n",
    "  model = Sequential()\n",
    "  # Flatten the input sequence to be compatible with Dense layers\n",
    "  model.add(Flatten(input_shape=(sequence_length-1, EMBEDDING_SIZE)))\n",
    "  # Add one or more Dense layers\n",
    "  model.add(Dense(128, activation='softmax'))\n",
    "  model.add(Dense(units=len(index_to_embedding), activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  history = model.fit(x=generator, steps_per_epoch=len(X) // batch_size, epochs=epochs)\n",
    "  training_loss_history = history.history['loss']\n",
    "  plt.plot(training_loss_history)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Training Loss')\n",
    "  plt.title('Training Loss Over Epochs')\n",
    "  plt.show()\n",
    "  model.save('../models/feed_forward_model.keras')\n",
    "  \n",
    "  return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14990/14990 [==============================] - 4615s 308ms/step - loss: 5.0818 - accuracy: 0.1893\n"
     ]
    }
   ],
   "source": [
    "our_feed_forward_model = build_feed_forward_model(X_train, y_train, index_to_embedding, batch_size=1000, sequence_length=N_GRAM, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(X, y, index_to_embedding: dict, batch_size=4, sequence_length=N_GRAM, epochs=1, units=16):\n",
    "    generator = data_generator(X, y, index_to_embedding, batch_size, sequence_length, epochs)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units, input_shape=(sequence_length-1, EMBEDDING_SIZE)))\n",
    "    model.add(Dense(len(index_to_embedding), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(x=generator, steps_per_epoch=len(X) // batch_size, epochs=epochs)\n",
    "    model.save('../models/rnn_model_units' + str(units)'.h5')\n",
    "    training_loss_history = history.history['loss']\n",
    "    plt.plot(training_loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.show()\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2215/10493 [=====>........................] - ETA: 35:21 - loss: 4.0306 - accuracy: 0.2413"
     ]
    }
   ],
   "source": [
    "rnn_model, rnn_history = build_rnn_model(X_train, y_train, index_to_embedding, batch_size=1000, sequence_length=N_GRAM, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(X, y, index_to_embedding: dict, batch_size=4, sequence_length=N_GRAM, epochs=1, ):\n",
    "    generator = data_generator(X, y, index_to_embedding, batch_size, sequence_length, epochs)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(sequence_length-1, EMBEDDING_SIZE)))\n",
    "    model.add(Dense(len(index_to_embedding), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(x=generator, steps_per_epoch=len(X) // batch_size, epochs=epochs)\n",
    "    model.save('../models/lstm_model.h5')\n",
    "    training_loss_history = history.history['loss']\n",
    "    plt.plot(training_loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.show()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model, lstm_history = build_lstm_model(X_train, y_train, index_to_embedding, batch_size=1000, sequence_length=N_GRAM, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "lstm_model.save('../models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attention_lstm_model(X, y, index_to_embedding: dict, batch_size=4, sequence_length=N_GRAM, epochs=1):\n",
    "    generator = data_generator(X, y, index_to_embedding, batch_size, sequence_length, epochs)\n",
    "    # Build the model\n",
    "    # model = Sequential()\n",
    "    # # Input layer\n",
    "    inputs = Input(shape=(sequence_length-1, EMBEDDING_SIZE))\n",
    "    # # LSTM layer with return_sequences=True to get the full sequence\n",
    "    lstm_out = LSTM(128, return_sequences=True)(inputs)\n",
    "\n",
    "    # # Attention mechanism\n",
    "    attention = Attention()([lstm_out, lstm_out])\n",
    "\n",
    "    # # Concatenate the LSTM output and the attention output\n",
    "    merged = Concatenate(axis=-1)([lstm_out, attention])\n",
    "\n",
    "    # Flatten the input\n",
    "    flattened = Flatten()(merged)\n",
    "\n",
    "    # # Dense layer for classification\n",
    "    output = Dense(len(index_to_embedding), activation='softmax')(flattened)\n",
    "\n",
    "    # # Build the model\n",
    "    # model = Model(inputs=inputs, outputs=output)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # # Fit the model\n",
    "    history = model.fit(x=generator, steps_per_epoch=len(X) // batch_size, epochs=epochs)\n",
    "\n",
    "    # # Save the model\n",
    "    model.save('../models/attention_lstm_model.h5')\n",
    "    training_loss_history = history.history['loss']\n",
    "    plt.plot(training_loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.show()\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 09:32:52.418043: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14990/14990 [==============================] - 14094s 940ms/step - loss: 0.1451 - accuracy: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "attention_lstm_model, attention_lstm_history = build_attention_lstm_model(X_train, y_train, index_to_embedding, batch_size=1000, sequence_length=N_GRAM, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Our Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_5 (SimpleRNN)    (None, 128)               29312     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 87923)             11342067  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11371379 (43.38 MB)\n",
      "Trainable params: 11371379 (43.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_embeddings = convertSamplesToEmbeddings(X_test, index_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = rnn_model.evaluate(np.array(X_test_embeddings), np.array(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
